#!/usr/bin/env python3
import subprocess
import re
import os
import random
import string
import pickle
import progressbar
import nltk.translate.ibm2 as align
import numpy as np
import pandas as pd
from readers.read_blast import BlastReader
from readers.read_giza import GIZAReader
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import KFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.linear_model import Perceptron
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

BLAST_PATH = '/home/marciolima/Documentos/Lalic/post-editing/src/error_identification/error-ident-blast.txt'
# BLAST_PATH = '/home/marciolima/Documentos/Lalic/post-editing/src/error_identification/exemplo.blast'
FEATURES_FILE = 'features_final.pkl'
TW_SZ = 5
ERRORS = ['lex-incTrWord', 'lex-notTrWord']

lb = LabelEncoder()


def tag_sentences(src, sys):
    """Tags all sentences from src and sys

    Arguments:
        src {list} -- Source language lines
        sys {list} -- Machine Translation output lines

    Returns:
        list -- List of tuples with aligned tagged lines. Tags are in a list
    """
    src_out = run_apertium_tagger(src, 'en')
    sys_out = run_apertium_tagger(sys, 'pt')

    assert len(src_out) == len(sys_out)
    num_sents = len(src_out)

    src_tags = list()
    sys_tags = list()
    for i in range(num_sents):
        src_tokens = re.findall(r'\^([^$]*)\$', src_out[i])
        sys_tokens = re.findall(r'\^([^$]*)\$', sys_out[i])

        # Replace crf tokens with $
        src_tokens = ['$' if w == 'cfr' else w for w in src_tokens]
        sys_tokens = ['$' if w == 'cfr' else w for w in sys_tokens]

        src_tags.append([re.findall(r'([^<>]+|^>$)', token)
                         for token in src_tokens])
        sys_tags.append([re.findall(r'([^<>]+|^>$)', token)
                         for token in sys_tokens])

    return list(zip(src_tags, sys_tags))


def run_apertium_tagger(lines, lang):
    """Creates a subprocess to run the Apertium Tagger via the `apertium/parse_file.sh` script

    Arguments:
        lines {list} -- List of lines to be tagged
        lang {str} -- Language to run the tagger with

    Returns:
        list -- Returns a list with all tagged lines in the Apertium format
    """
    application_path = str(os.path.abspath(os.path.curdir))
    out_lines = list()

    for line in progressbar.progressbar(lines):
        proc = subprocess.Popen([application_path + '/apertium/parse_file.sh', '--lang', lang],
                                stdin=subprocess.PIPE, universal_newlines=True,
                                stdout=subprocess.PIPE)
        out = proc.communicate(' '.join(line))
        out_lines.append(out[0])
    return out_lines


def extract_features(tagged_sent, alignment, tw_size, target):
    """Generates the features for the identification given the tagged sentence pair,
    the alignment between them, the window size and a specification of the error
    corresponding to them

    Arguments:
        tagged_sent {tuple} -- Tuple from the list generated by the `tag_sentences` method
        alignment {list} -- List of tuples with the alignment between the setences
        tw_size {int} -- Window size for the computation of the features
        target {str} -- Error corresponding to the sentence,
                        if the translation is correct this parameter must be valued to `correct`

    Returns:
        dict -- Dictionary with all features to the sentence pair
    """
    tagged_src = tagged_sent[0]
    tagged_sys = tagged_sent[1]

    lct_src_index = list()
    lct_sys_index = list()

    # Apertium deals with MWE
    # Indices where one has MWE
    has_mw_src = [(i, w[0].count(' '))
                  for (i, w) in enumerate(tagged_src) if ' ' in w[0]]
    has_mw_sys = [(i, w[0].count(' '))
                  for (i, w) in enumerate(tagged_sys) if ' ' in w[0]]

    # Get LCT
    if target == 'correct':
        while not lct_sys_index or None in lct_sys_index:
            lct_src_index = random.choice(range(len(tagged_src)))
            # Get sys LCT by alignment
            lct_sys_index = [a[1] for a in alignment if a[0] == lct_src_index]

        # Correct alignment of tokens in case of MWE
        if has_mw_src:
            for (i, num_spaces) in has_mw_src:
                lct_src_index = [lct if lct <= i else lct -
                                 min(num_spaces, lct - i) for lct in [lct_src_index]][0]
        if has_mw_sys:
            for (i, num_spaces) in has_mw_sys:
                lct_sys_index = [lct if lct <= i else lct -
                                 min(num_spaces, lct - i) for lct in lct_sys_index]

        if len(lct_sys_index) < 3:
            lct_sys_index = lct_sys_index[0]
        else:
            lct_sys_index = lct_sys_index[len(lct_sys_index) // 2]
    else:
        lct_src_index = target[0]
        lct_sys_index = target[1]

        # If BLAST aligned word is after the MWE
        # Needs to discount how many spaces are being ignored in the tagged sentences
        if has_mw_src:
            for (i, num_spaces) in has_mw_src:
                lct_src_index = [lct if lct <=
                                 i else lct - num_spaces for lct in lct_src_index]
        if has_mw_sys:
            for (i, num_spaces) in has_mw_sys:
                lct_sys_index = [lct if lct <=
                                 i else lct - num_spaces for lct in lct_sys_index]

        # Ignore sentences without alignment
        if -1 in lct_src_index:
            return None
        if -1 in lct_sys_index:
            return None

        if len(lct_sys_index) < 3:
            lct_sys_index = lct_sys_index[0]
        else:
            lct_sys_index = lct_sys_index[len(lct_sys_index) // 2]

        if len(lct_src_index) < 3:
            lct_src_index = lct_src_index[0]
        else:
            lct_src_index = lct_src_index[len(lct_src_index) // 2]

    # Get TW
    src_tw_start = max(lct_src_index - (tw_size // 2), 0)
    src_tw_end = min(lct_src_index + (tw_size // 2) + 1, len(tagged_src))
    src_tw = tagged_src[src_tw_start: src_tw_end]

    sys_tw_start = max(lct_sys_index - (tw_size // 2), 0)
    sys_tw_end = min(lct_sys_index + (tw_size // 2) + 1, len(tagged_sys))
    sys_tw = tagged_sys[sys_tw_start: sys_tw_end]

    # Compute features
    features = dict()

    # Features src
    features['srcSize'] = len(tagged_src)
    features['engPossessive'] = bool([w for w in tagged_src if 'pos' in w])

    # Features sys
    features['sysSize'] = len(tagged_sys)

    for i in range(tw_size):
        try:
            tok = sys_tw[i]
        except IndexError:
            tok = list()
        finally:
            # Gender
            key = ''
            if i < tw_size // 2:
                key = 'genToken{}BefSys'.format(i + 1)
            elif i == tw_size // 2:
                key = 'genSysToken'
            else:
                key = 'genToken{}AftSys'.format(i - (tw_size // 2))
            features[key] = 'f' if 'f' in tok else 'm' if 'm' in tok else 'NC'

            # Number
            if i < tw_size // 2:
                key = 'numToken{}BefSys'.format(i + 1)
            elif i == tw_size // 2:
                key = 'numSysToken'
            else:
                key = 'numToken{}AftSys'.format(i - (tw_size // 2))
            features[key] = 'sg' if 'sg' in tok else 'pl' if 'pl' in tok else 'NC'

    features['SysTokenTag'] = True if len(
        tagged_sys[lct_sys_index]) > 1 else False
    features['sysBegCap'] = True if tagged_sys[lct_sys_index][0][0].isupper() else False

    # Features sys and src
    for i in range(tw_size):
        try:
            tok = src_tw[i]
        except IndexError:
            tok = list()
        finally:
            key = ''
            tok_tags = re.sub(r'_\+[^_$]+_', '+', '_'.join(tok[1:]))
            if i < tw_size // 2:
                key = 'posToken{}BefSrc'.format(i + 1)
            elif i == tw_size // 2:
                key = 'posSrcToken'
            else:
                key = 'posToken{}AftSrc'.format(i - (tw_size // 2))
            features[key] = tok_tags if tok_tags else 'NC'

    for i in range(tw_size):
        try:
            tok = sys_tw[i]
        except:
            tok = list()
        finally:
            key = ''
            tok_tags = re.sub(r'_\+[^_$]+_', '+', '_'.join(tok[1:]))
            if i < tw_size // 2:
                key = 'posToken{}BefSys'.format(i + 1)
            elif i == tw_size // 2:
                key = 'posSysToken'
            else:
                key = 'posToken{}AftSys'.format(i - (tw_size // 2))
            features[key] = tok_tags if tok_tags else 'NC'

    try:
        if re.match('vb[^_]+', '_'.join(tagged_src[lct_src_index][1:])):
            features['verbSrc'] = re.sub(
                'vb[^_]+_', '', '_'.join(tagged_src[lct_src_index][1:]))
        else:
            features['verbSrc'] = 'NC'
    except IndexError:
        features['verbSrc'] = 'NC'

    try:
        if re.match('vb[^_]+', '_'.join(tagged_sys[lct_sys_index][1:])):
            features['verbSys'] = re.sub(
                'vb[^_]+_', '', '_'.join(tagged_sys[lct_sys_index][1:]))
        else:
            features['verbSys'] = 'NC'
    except IndexError:
        features['verbSys'] = 'NC'

    # Features sys/src
    features['srcSysRatio'] = len(tagged_src) / len(tagged_sys)

    num_verbs_src = 0
    num_nouns_src = 0
    for tok in tagged_src:
        try:
            if re.match('vb[^_]+', '_'.join(tok[1:])):
                num_verbs_src += 1
            elif 'n' in tok[1:]:
                num_nouns_src += 1
        except IndexError:
            pass

    num_verbs_sys = 0
    num_nouns_sys = 0
    for tok in tagged_sys:
        try:
            if re.match('vb[^_]+', '_'.join(tok[1:])):
                num_verbs_sys += 1
            elif 'n' in tok[1:]:
                num_nouns_sys += 1
        except IndexError:
            pass

    features['srcSysVRatio'] = max(num_verbs_src, 1) / max(num_verbs_sys, 1)
    features['srcSysNRatio'] = max(num_nouns_src, 1) / max(num_nouns_sys, 1)

    # Remove * from start of lct if exists
    flat_lct_src = re.sub(r'\*', '', tagged_src[lct_src_index][0])
    flat_lct_sys = re.sub(r'\*', '', tagged_sys[lct_sys_index][0])

    features['equalTokenSrcSys'] = flat_lct_src == flat_lct_sys
    special_char_src = flat_lct_src in string.punctuation or 'num' in tagged_src[
        lct_src_index]
    special_char_sys = flat_lct_sys in string.punctuation or 'num' in tagged_sys[
        lct_sys_index]
    features['specialCharSrcSys'] = special_char_src and special_char_sys

    # Include target to the features dictionary
    features['target'] = target

    return features


def format_features(features):
    features_names = list(features[0].keys())
    features_names.remove('target')  # Ignore target for now

    data = pd.DataFrame(features, columns=features_names)

    # Get in DataFrame only numeric and boolean columns
    # String columns are stored in string_cols
    string_cols = data.select_dtypes(include='object')
    data = data.select_dtypes(exclude='object')

    # For each column with string features
    # Split with '_' and code with get_dummies
    # Then join to the main DataFrame
    for col in string_cols:
        nova_coluna = pd.get_dummies(string_cols[col].str.split('_').apply(
            pd.Series).stack(), prefix=col, prefix_sep='_').sum(level=0)
        data = data.join(nova_coluna)

    # Add target column
    data = data.join(pd.DataFrame(features, columns=['target']))

    # Get only error label in the target column
    error_cols = data.loc[data['target'] != 'correct', 'target']
    error_cols = error_cols.apply(pd.Series)[3]
    data.loc[data['target'] != 'correct', 'target'] = error_cols

    # Replace not correct targets with error
    data.loc[data['target'] != 'correct', 'target'] = 'error'
    # data['target'] = data['target'].str.replace("-.*$", "")

    # Encode target into numbers
    data['target'] = lb.fit_transform(data['target'])

    return data


def test_kfold_methods(data):
    X = data.loc[:, data.columns != 'target']
    y = data['target']

    kf = KFold(n_splits=10, shuffle=True)

    print('Arvore de decisao')
    avg_precision = 0
    avg_precision_correct = 0
    avg_precision_error = 0
    fold = 1
    for (train, test) in kf.split(X):
        # Training
        dt = DecisionTreeClassifier()
        dt.fit(X.loc[train], y.loc[train])

        # print(X.loc[train].loc[y.loc[train] != 0])

        results = dt.predict(X.loc[test])
        precision = accuracy_score(y.loc[test], results)
        avg_precision += precision

        precision_correct = accuracy_score(
            y.loc[test].loc[y.loc[test] == 0], results[y.loc[test] == lb.transform(['correct'])[0]])
        avg_precision_correct += precision_correct
        precision_error = accuracy_score(
            y.loc[test].loc[y.loc[test] != 0], results[y.loc[test] != lb.transform(['correct'])[0]])
        avg_precision_error += precision_error

        print('Precisao - Fold {}: {:.2f}%'.format(fold, precision * 100))
        print('Precisao corretas - Fold {}: {:.2f}%'.format(fold,
                                                            precision_correct * 100))
        print('Precisao erros - Fold {}: {:.2f}%\n'.format(fold, precision_error * 100))
        fold += 1
    avg_precision /= 10
    avg_precision_correct /= 10
    avg_precision_error /= 10
    print('Precisao media: {:.2f}%'.format(avg_precision * 100))
    print('Precisao media corretas: {:.2f}%'.format(
        avg_precision_correct * 100))
    print('Precisao media erros: {:.2f}%'.format(avg_precision_error * 100))
    print('------------------------------')

    print('SVM')
    avg_precision = 0
    avg_precision_correct = 0
    avg_precision_error = 0
    fold = 1
    for (train, test) in kf.split(X):
        svm = SVC()
        svm.fit(X.loc[train], y.loc[train])

        results = svm.predict(X.loc[test])
        precision = accuracy_score(y.loc[test], results)
        avg_precision += precision

        precision_correct = accuracy_score(
            y.loc[test].loc[y.loc[test] == 0], results[y.loc[test] == lb.transform(['correct'])[0]])
        avg_precision_correct += precision_correct
        precision_error = accuracy_score(
            y.loc[test].loc[y.loc[test] != 0], results[y.loc[test] != lb.transform(['correct'])[0]])
        avg_precision_error += precision_error

        print('Precisao - Fold {}: {:.2f}%'.format(fold, precision * 100))
        print('Precisao corretas - Fold {}: {:.2f}%'.format(fold,
                                                            precision_correct * 100))
        print('Precisao erros - Fold {}: {:.2f}%\n'.format(fold, precision_error * 100))
        fold += 1
    avg_precision /= 10
    avg_precision_correct /= 10
    avg_precision_error /= 10
    print('Precisao media: {:.2f}%'.format(avg_precision * 100))
    print('Precisao media corretas: {:.2f}%'.format(
        avg_precision_correct * 100))
    print('Precisao media erros: {:.2f}%'.format(avg_precision_error * 100))
    print('------------------------------')

    print('Perceptron')
    avg_precision = 0
    avg_precision_correct = 0
    avg_precision_error = 0
    fold = 1
    for (train, test) in kf.split(X):
        perceptron = Perceptron()
        perceptron.fit(X.loc[train], y.loc[train])

        results = perceptron.predict(X.loc[test])
        precision = accuracy_score(y.loc[test], results)
        avg_precision += precision

        precision_correct = accuracy_score(
            y.loc[test].loc[y.loc[test] == 0], results[y.loc[test] == lb.transform(['correct'])[0]])
        avg_precision_correct += precision_correct
        precision_error = accuracy_score(
            y.loc[test].loc[y.loc[test] != 0], results[y.loc[test] != lb.transform(['correct'])[0]])
        avg_precision_error += precision_error

        print('Precisao - Fold {}: {:.2f}%'.format(fold, precision * 100))
        print('Precisao corretas - Fold {}: {:.2f}%'.format(fold,
                                                            precision_correct * 100))
        print('Precisao erros - Fold {}: {:.2f}%\n'.format(fold, precision_error * 100))
        fold += 1
    avg_precision /= 10
    avg_precision_correct /= 10
    avg_precision_error /= 10
    print('Precisao media: {:.2f}%'.format(avg_precision * 100))
    print('Precisao media corretas: {:.2f}%'.format(
        avg_precision_correct * 100))
    print('Precisao media erros: {:.2f}%'.format(avg_precision_error * 100))
    print('------------------------------')

    print('Random Forest')
    avg_precision = 0
    avg_precision_correct = 0
    avg_precision_error = 0
    fold = 1
    for (train, test) in kf.split(X):
        random_forest = RandomForestClassifier()
        random_forest.fit(X.loc[train], y.loc[train])

        results = random_forest.predict(X.loc[test])
        precision = accuracy_score(y.loc[test], results)
        avg_precision += precision

        precision_correct = accuracy_score(
            y.loc[test].loc[y.loc[test] == 0], results[y.loc[test] == lb.transform(['correct'])[0]])
        avg_precision_correct += precision_correct
        precision_error = accuracy_score(
            y.loc[test].loc[y.loc[test] != 0], results[y.loc[test] != lb.transform(['correct'])[0]])
        avg_precision_error += precision_error

        print('Precisao - Fold {}: {:.2f}%'.format(fold, precision * 100))
        print('Precisao corretas - Fold {}: {:.2f}%'.format(fold,
                                                            precision_correct * 100))
        print('Precisao erros - Fold {}: {:.2f}%\n'.format(fold, precision_error * 100))
        fold += 1
    avg_precision /= 10
    avg_precision_correct /= 10
    avg_precision_error /= 10
    print('Precisao media: {:.2f}%'.format(avg_precision * 100))
    print('Precisao media corretas: {:.2f}%'.format(
        avg_precision_correct * 100))
    print('Precisao media erros: {:.2f}%'.format(avg_precision_error * 100))


def main():
    """Main function
    """
    blast_reader = BlastReader(BLAST_PATH)
    src_lines = list()
    sys_lines = list()
    target = list()

    # Files for GIZA
    src_file = open('/tmp/src.txt', 'w')
    sys_file = open('/tmp/sys.txt', 'w')

    # Correct sentences
    for i in blast_reader.get_correct_indices():
        src_lines.append(blast_reader.src_lines[i])
        sys_lines.append(blast_reader.sys_lines[i])
        target.append('correct')

        # Write files for GIZA
        src_file.write(' '.join(blast_reader.src_lines[i]))
        src_file.write('\n')
        sys_file.write(' '.join(blast_reader.sys_lines[i]))
        sys_file.write('\n')

    # Error lines
    errors = blast_reader.get_filtered_errors(ERRORS)
    # errors = blast_reader.error_lines
    for (line, error) in errors:
        src_lines.append(blast_reader.src_lines[line])
        sys_lines.append(blast_reader.sys_lines[line])
        target.append(error)

        src_file.write(' '.join(blast_reader.src_lines[line]))
        src_file.write('\n')
        sys_file.write(' '.join(blast_reader.sys_lines[line]))
        sys_file.write('\n')
    src_file.close()
    sys_file.close()

    # Tag sentences
    print('Tagging sentences')
    tagged_lines = tag_sentences(src_lines, sys_lines)

    # Align sentences
    print('Aligning sentences')
    application_path = str(os.path.abspath(os.path.curdir))
    proc = subprocess.Popen([application_path + '/aligner/align_sentences.sh',
                             '--srcpath', '/tmp/src.txt',
                             '--syspath', '/tmp/sys.txt'],
                            stdout=subprocess.PIPE)
    out = proc.communicate()
    num_sents = int(out[0])
    giza_reader = GIZAReader('/tmp/giza.output')
    alignments = giza_reader.aligned_lines[:num_sents]

    # Extract features
    print('Extracting features')
    training_instances = list()
    ignored_instances = 0
    for (i, sent) in progressbar.progressbar(enumerate(tagged_lines)):
        features = extract_features(
            sent, alignments[i]['alignment'], TW_SZ, target[i])
        if features:
            training_instances.append(features)
        else:
            ignored_instances += 1
    print('Finalizado!')
    print('Instancias ignoradas: {}'.format(ignored_instances))

    print('Iniciando treinamento')
    data = format_features(training_instances)
    test_kfold_methods(data)


if __name__ == '__main__':
    main()
